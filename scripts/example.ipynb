{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import automatic_speech_recognition as asr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/blue/vbindschadler/hadi10102/train_data/speech_commands/'\n",
    "\n",
    "file_paths = asr.util.preprocess.read_simple_word(data_dir)\n",
    "\n",
    "# Split data into train, val, and test\n",
    "train_files_paths = file_paths[:6400]\n",
    "val_files_paths = file_paths[6400: 6400 + 800]\n",
    "test_files_paths = file_paths[-800:]\n",
    "\n",
    "# Reads the audio data for the file_paths\n",
    "train_ds = asr.util.preprocess.preprocess_simple_word(train_files_paths)\n",
    "val_ds = asr.util.preprocess.preprocess_simple_word(val_files_paths)\n",
    "test_ds = asr.util.preprocess.preprocess_simple_word(test_files_paths)\n",
    "\n",
    "# Padd the data sets\n",
    "train_ds_padded = train_ds.padded_batch(8, padded_shapes={'audio_input': (None, 1), \n",
    "                                                          'y_true' : (None,),\n",
    "                                                          'y_true_length' :()})\n",
    "\n",
    "val_ds_padded = val_ds.padded_batch(8, padded_shapes={'audio_input': (None, 1),\n",
    "                                                      'y_true' : (None,),\n",
    "                                                      'y_true_length' :()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model input and output layers\n",
    "logits, input_audio, y_true, y_true_length = asr.models.cnn_raw_speech.cnn_raw_speech.get_model()\n",
    "\n",
    "# Setup the CTC loss layer\n",
    "ctc_loss_layer = asr.util.ctc_loss.get_ctc_layer(logits, y_true, y_true_length)\n",
    "\n",
    "# Setup Model and training params\n",
    "model = tf.keras.Model(inputs = [input_audio, y_true, y_true_length], outputs = [ctc_loss_layer, logits])\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='min')\n",
    "\n",
    "model.fit(x = train_ds_padded, \n",
    "          validation_data = val_ds_padded, \n",
    "          shuffle = True,\n",
    "          callbacks = [checkpoint, early_stop],\n",
    "          epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepspeech_kernel",
   "language": "python",
   "name": "deepspeech_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
